Namespace(batch_size=64, config='configs/ATIS/GraSP/SCD.yml', d_inner_hid=2048, d_k=64, d_model=768, d_v=64, dropout=0.1, embs_share_weight=False, epoch=100, label_smoothing=True, log=None, n_head=8, n_layers=6, n_warmup_steps=40000, no_cuda=False, ops_idx=None, precondition=0, proj_share_weight=True, quantized=1, save_mode='best', save_model=None, src_vocab_size=19213, tensorized=0, tgt_vocab_size=10839, uncompressed=0)
Transformer_sentence_concat_SLU(
  (encoder): Encoder(
    (src_word_emb): Embedding(800, 768)
    (position_enc): Embedding(512, 768)
    (token_type_emb): Embedding(2, 768)
    (dropout): Dropout(p=0.1, inplace=False)
    (layer_stack): ModuleList(
      (0): EncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=768, out_features=768, bias=True)
          (w_ks): Linear(in_features=768, out_features=768, bias=True)
          (w_vs): Linear(in_features=768, out_features=768, bias=True)
          (fc): Linear(in_features=768, out_features=768, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=3072, bias=True)
          (w_2): Linear(in_features=3072, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (relu): GELU(approximate='none')
        )
      )
      (1): EncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=768, out_features=768, bias=True)
          (w_ks): Linear(in_features=768, out_features=768, bias=True)
          (w_vs): Linear(in_features=768, out_features=768, bias=True)
          (fc): Linear(in_features=768, out_features=768, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=3072, bias=True)
          (w_2): Linear(in_features=3072, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (relu): GELU(approximate='none')
        )
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
  )
  (preclassifier): Linear(in_features=768, out_features=768, bias=True)
  (slot_preclassifier): Linear(in_features=768, out_features=768, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=768, out_features=768, bias=True)
    (1): Tanh()
    (2): Dropout(p=0.1, inplace=False)
    (3): Linear(in_features=768, out_features=22, bias=True)
  )
  (slot_classifier): Sequential(
    (0): Linear(in_features=768, out_features=768, bias=True)
    (1): GELU(approximate='none')
    (2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (3): Linear(in_features=768, out_features=121, bias=True)
  )
)
0
=> config.summary_dir:    ./runs/pruning/ATIS/attn/ZO_SCD_mask/ATIS_GraSP_SCD_95_incre/run_None/summary/
=> config.checkpoint_dir: ./runs/pruning/ATIS/attn/ZO_SCD_mask/ATIS_GraSP_SCD_95_incre/run_None/checkpoint/
{'Total': 16479119, 'Trainable': 16479119}
{'encoder.layer_stack.0.slf_attn.w_qs': {'weight': tensor([  1680,   1883,   1997,  ..., 589389, 589509, 589766], device='cuda:0')}, 'encoder.layer_stack.0.slf_attn.w_ks': {'weight': tensor([   827,    981,   1042,  ..., 587505, 588475, 588679], device='cuda:0')}, 'encoder.layer_stack.0.slf_attn.w_vs': {'weight': tensor([     0,      4,      9,  ..., 589817, 589821, 589822], device='cuda:0')}, 'encoder.layer_stack.0.slf_attn.fc': {'weight': tensor([    72,    127,    132,  ..., 589783, 589785, 589800], device='cuda:0')}, 'encoder.layer_stack.0.pos_ffn.w_1': {'weight': tensor([     12,      50,     225,  ..., 2359029, 2359164, 2359272],
       device='cuda:0')}, 'encoder.layer_stack.0.pos_ffn.w_2': {'weight': tensor([     11,      46,     100,  ..., 2358991, 2359063, 2359162],
       device='cuda:0')}, 'encoder.layer_stack.1.slf_attn.w_qs': {'weight': tensor([  9209,   9267,   9285,   9333,   9387,   9431,   9464,   9500,   9529,
          9577,   9675,   9795,   9860,  10782,  10795,  10902,  10967,  11042,
         11159,  13937,  13966,  14019,  14185,  14189,  14238,  14244,  14249,
         14333,  14335,  14340,  14450,  14461,  14585,  18399,  20431,  20541,
         21157,  22289,  22633,  22819,  25337,  26121,  26126,  26163,  26229,
         26252,  26362,  26366,  26421,  26478,  26665,  26738,  26873,  28586,
         32441,  32470,  32812,  33962,  33966,  34428,  36281,  36509,  36594,
         36770,  36826,  37669,  37707,  40121,  41507,  41612,  42738,  42832,
         44743,  45053,  45345,  45449,  45566,  45621,  45750,  45751,  45826,
         53009,  53293,  61789,  69262,  69429,  77738,  95681, 108855, 108914,
        115140, 115836, 129876, 130007, 130046, 130202, 130273, 130329, 130475,
        130553, 137114, 137126, 137465, 137917, 138328, 138382, 138531, 138579,
        138598, 138791, 138914, 139416, 139575, 139581, 141344, 141606, 141659,
        141725, 141760, 141948, 141986, 146763, 149806, 150073, 150113, 150146,
        150258, 150499, 152935, 152972, 153538, 155934, 155955, 156089, 156119,
        156532, 156612, 156642, 159064, 159401, 160696, 161212, 167639, 167720,
        167740, 167817, 168163, 170063, 170084, 173598, 173924, 174005, 174087,
        175244, 175274, 175677, 176498, 194355, 194741, 194969, 195004, 208489,
        217529, 217651, 245025, 269738, 272009, 278265, 278287, 281139, 281198,
        281232, 281501, 281509, 281513, 281724, 288043, 288339, 288407, 288498,
        288643, 288741, 294345, 294435, 294446, 294510, 294563, 294564, 294581,
        294615, 294633, 294635, 294700, 294713, 294723, 294824, 294850, 294872,
        295589, 300527, 300709, 303948, 307243, 307385, 307487, 307659, 307930,
        309555, 309791, 312606, 313074, 313149, 328492, 334275, 334754, 334899,
        335018, 335022, 335214, 335269, 335537, 335870, 336024, 336292, 347307,
        348721, 348782, 349062, 349223, 349345, 349362, 351027, 353383, 353390,
        353986, 354222, 354486, 354585, 355155, 355522, 358698, 358890, 359008,
        359069, 359081, 359330, 359400, 360362, 360614, 365989, 366060, 366242,
        366274, 367975, 367989, 368156, 368182, 368208, 368359, 368510, 368621,
        371815, 372025, 372279, 373390, 373569, 373785, 374009, 377925, 378482,
        378594, 384806, 385089, 385120, 385341, 386683, 386741, 399768, 408886,
        421490, 421570, 421608, 421774, 422089, 422137, 422585, 422814, 423144,
        428312, 433950, 434057, 434165, 434365, 434471, 434546, 434650, 454249,
        459573, 459756, 460974, 461039, 461074, 461158, 461161, 461237, 461351,
        461507, 462903, 465433, 465964, 466082, 473496, 473586, 473624, 474264,
        474448, 474545, 474562, 475061, 478269, 479117, 483212, 483292, 483391,
        483479, 483517, 483564, 483653, 483778, 483797, 486004, 489015, 496139,
        514997, 517902, 518130, 518393, 521910, 523145, 523182, 523256, 523682,
        530556, 530862, 530997, 531108, 531152, 533931, 534069, 534219, 535307,
        537184, 547799, 548136, 553868, 556605, 562189, 562310, 562537, 562584,
        562937, 563897], device='cuda:0')}, 'encoder.layer_stack.1.slf_attn.w_ks': {'weight': tensor([  9300,   9332,   9566,   9608,   9825,  10098,  10283,  10294,  10340,
         10362,  10501,  10507,  10625,  10722,  13874,  13908,  14202,  14347,
         14704,  15214,  17195,  17198,  17596,  19273,  19980,  20041,  20267,
         20353,  20461,  23820,  24428,  26044,  27747,  28430,  28440,  28467,
         28500,  28801,  28847,  28874,  28899,  28900,  28962,  31243,  31671,
         31981,  32972,  34644,  34761,  34864,  35021,  35617,  37072,  39261,
         39388,  39495,  39506,  39530,  39546,  39553,  39560,  39566,  39613,
         39723,  39749,  42291,  42530,  42550,  42590,  42786,  42819,  49122,
         56316,  56429,  56525,  56635,  67799,  68156,  75272,  75314,  75441,
         75744,  75951,  89662,  90744,  90835,  91200,  92936,  94176, 100264,
        100369, 103072, 103464, 109985, 113394, 113596, 113869, 114317, 122767,
        128396, 128434, 128808, 128872, 128956, 132146, 134898, 137642, 137656,
        137673, 137712, 137859, 137970, 138204, 139181, 139219, 139504, 139527,
        139553, 139606, 142241, 142364, 142389, 142418, 142432, 142447, 142504,
        142542, 142617, 142632, 142674, 142712, 142771, 142835, 142839, 143793,
        144009, 144114, 149004, 150172, 150503, 152293, 152485, 152554, 152572,
        152599, 152684, 152712, 156069, 156091, 156166, 156284, 156328, 156329,
        156352, 156377, 156926, 157194, 158666, 158776, 158828, 163368, 166821,
        167405, 167534, 167745, 168972, 169063, 169614, 170957, 177710, 178084,
        179198, 179658, 179708, 182664, 184131, 186424, 187914, 194403, 194418,
        194447, 194464, 194615, 194777, 194920, 194980, 195051, 203661, 203669,
        204230, 206784, 207115, 212629, 221258, 241675, 244180, 244885, 245203,
        245419, 254220, 263014, 289957, 297673, 298130, 298157, 298189, 298327,
        298669, 309887, 310450, 314972, 314974, 314997, 315164, 316608, 316626,
        316732, 316767, 317111, 317376, 317541, 317828, 317931, 318949, 330765,
        333410, 334131, 334272, 334470, 334616, 339924, 340385, 340508, 340657,
        340674, 346002, 346056, 351023, 351096, 351672, 352632, 353138, 353406,
        353453, 353624, 353671, 354237, 356048, 357318, 357366, 358753, 358780,
        358788, 358899, 358902, 358934, 359065, 359236, 359328, 362634, 371408,
        372766, 372980, 375838, 376440, 377214, 382917, 382976, 383279, 383405,
        384060, 384097, 384198, 384260, 384423, 390983, 391032, 402480, 426151,
        426529, 433063, 436903, 445817, 445864, 460483, 460593, 460613, 467874,
        468338, 468690, 469228, 473954, 474485, 474605, 479330, 479797, 479875,
        479890, 479908, 479955, 482506, 482731, 482932, 482948, 483234, 483434,
        483508, 483677, 483808, 483819, 483893, 483904, 484026, 486329, 486336,
        486425, 486439, 486445, 486592, 486653, 486696, 486726, 486894, 492310,
        493012, 493191, 495016, 499397, 499624, 499928, 501717, 506308, 506330,
        506347, 506450, 506573, 506648, 514654, 520726, 525643, 526921, 527021,
        527272, 527342, 527375, 527394, 527451, 527456, 527457, 531019, 533004,
        533243, 533502, 547051, 548517, 548748, 565880, 567604, 567649, 567763,
        567850, 568005, 568248, 584453, 584502, 584736, 588684],
       device='cuda:0')}, 'encoder.layer_stack.1.slf_attn.w_vs': {'weight': tensor([     0,      6,      8,  ..., 589816, 589817, 589822], device='cuda:0')}, 'encoder.layer_stack.1.slf_attn.fc': {'weight': tensor([    13,     16,     17,  ..., 589799, 589811, 589817], device='cuda:0')}, 'encoder.layer_stack.1.pos_ffn.w_1': {'weight': tensor([      5,     682,     770,  ..., 2359238, 2359285, 2359289],
       device='cuda:0')}, 'encoder.layer_stack.1.pos_ffn.w_2': {'weight': tensor([     41,      47,      53,  ..., 2359200, 2359250, 2359292],
       device='cuda:0')}, 'preclassifier': {'weight': tensor([   770,    771,    783,  ..., 589800, 589813, 589819], device='cuda:0')}, 'slot_preclassifier': {'weight': tensor([     0,      4,     13,  ..., 589078, 589481, 589565], device='cuda:0')}, 'classifier.3': {'weight': tensor([    1,     2,     6,  ..., 16878, 16880, 16888], device='cuda:0')}, 'slot_classifier.3': {'weight': tensor([   15,    17,    40,  ..., 92897, 92906, 92925], device='cuda:0')}}
{'encoder.layer_stack.0.slf_attn.w_qs': {'weight': tensor([  1680,   1883,   1997,  ..., 589389, 589509, 589766], device='cuda:0')}, 'encoder.layer_stack.0.slf_attn.w_ks': {'weight': tensor([   827,    981,   1042,  ..., 587505, 588475, 588679], device='cuda:0')}, 'encoder.layer_stack.0.slf_attn.w_vs': {'weight': tensor([     0,      4,      9,  ..., 589817, 589821, 589822], device='cuda:0')}, 'encoder.layer_stack.0.slf_attn.fc': {'weight': tensor([    72,    127,    132,  ..., 589783, 589785, 589800], device='cuda:0')}, 'encoder.layer_stack.0.pos_ffn.w_1': {'weight': tensor([     12,      50,     225,  ..., 2359029, 2359164, 2359272],
       device='cuda:0')}, 'encoder.layer_stack.0.pos_ffn.w_2': {'weight': tensor([     11,      46,     100,  ..., 2358991, 2359063, 2359162],
       device='cuda:0')}, 'encoder.layer_stack.1.slf_attn.w_qs': {'weight': tensor([  9209,   9267,   9285,   9333,   9387,   9431,   9464,   9500,   9529,
          9577,   9675,   9795,   9860,  10782,  10795,  10902,  10967,  11042,
         11159,  13937,  13966,  14019,  14185,  14189,  14238,  14244,  14249,
         14333,  14335,  14340,  14450,  14461,  14585,  18399,  20431,  20541,
         21157,  22289,  22633,  22819,  25337,  26121,  26126,  26163,  26229,
         26252,  26362,  26366,  26421,  26478,  26665,  26738,  26873,  28586,
         32441,  32470,  32812,  33962,  33966,  34428,  36281,  36509,  36594,
         36770,  36826,  37669,  37707,  40121,  41507,  41612,  42738,  42832,
         44743,  45053,  45345,  45449,  45566,  45621,  45750,  45751,  45826,
         53009,  53293,  61789,  69262,  69429,  77738,  95681, 108855, 108914,
        115140, 115836, 129876, 130007, 130046, 130202, 130273, 130329, 130475,
        130553, 137114, 137126, 137465, 137917, 138328, 138382, 138531, 138579,
        138598, 138791, 138914, 139416, 139575, 139581, 141344, 141606, 141659,
        141725, 141760, 141948, 141986, 146763, 149806, 150073, 150113, 150146,
        150258, 150499, 152935, 152972, 153538, 155934, 155955, 156089, 156119,
        156532, 156612, 156642, 159064, 159401, 160696, 161212, 167639, 167720,
        167740, 167817, 168163, 170063, 170084, 173598, 173924, 174005, 174087,
        175244, 175274, 175677, 176498, 194355, 194741, 194969, 195004, 208489,
        217529, 217651, 245025, 269738, 272009, 278265, 278287, 281139, 281198,
        281232, 281501, 281509, 281513, 281724, 288043, 288339, 288407, 288498,
        288643, 288741, 294345, 294435, 294446, 294510, 294563, 294564, 294581,
        294615, 294633, 294635, 294700, 294713, 294723, 294824, 294850, 294872,
        295589, 300527, 300709, 303948, 307243, 307385, 307487, 307659, 307930,
        309555, 309791, 312606, 313074, 313149, 328492, 334275, 334754, 334899,
        335018, 335022, 335214, 335269, 335537, 335870, 336024, 336292, 347307,
        348721, 348782, 349062, 349223, 349345, 349362, 351027, 353383, 353390,
        353986, 354222, 354486, 354585, 355155, 355522, 358698, 358890, 359008,
        359069, 359081, 359330, 359400, 360362, 360614, 365989, 366060, 366242,
        366274, 367975, 367989, 368156, 368182, 368208, 368359, 368510, 368621,
        371815, 372025, 372279, 373390, 373569, 373785, 374009, 377925, 378482,
        378594, 384806, 385089, 385120, 385341, 386683, 386741, 399768, 408886,
        421490, 421570, 421608, 421774, 422089, 422137, 422585, 422814, 423144,
        428312, 433950, 434057, 434165, 434365, 434471, 434546, 434650, 454249,
        459573, 459756, 460974, 461039, 461074, 461158, 461161, 461237, 461351,
        461507, 462903, 465433, 465964, 466082, 473496, 473586, 473624, 474264,
        474448, 474545, 474562, 475061, 478269, 479117, 483212, 483292, 483391,
        483479, 483517, 483564, 483653, 483778, 483797, 486004, 489015, 496139,
        514997, 517902, 518130, 518393, 521910, 523145, 523182, 523256, 523682,
        530556, 530862, 530997, 531108, 531152, 533931, 534069, 534219, 535307,
        537184, 547799, 548136, 553868, 556605, 562189, 562310, 562537, 562584,
        562937, 563897], device='cuda:0')}, 'encoder.layer_stack.1.slf_attn.w_ks': {'weight': tensor([  9300,   9332,   9566,   9608,   9825,  10098,  10283,  10294,  10340,
         10362,  10501,  10507,  10625,  10722,  13874,  13908,  14202,  14347,
         14704,  15214,  17195,  17198,  17596,  19273,  19980,  20041,  20267,
         20353,  20461,  23820,  24428,  26044,  27747,  28430,  28440,  28467,
         28500,  28801,  28847,  28874,  28899,  28900,  28962,  31243,  31671,
         31981,  32972,  34644,  34761,  34864,  35021,  35617,  37072,  39261,
         39388,  39495,  39506,  39530,  39546,  39553,  39560,  39566,  39613,
         39723,  39749,  42291,  42530,  42550,  42590,  42786,  42819,  49122,
         56316,  56429,  56525,  56635,  67799,  68156,  75272,  75314,  75441,
         75744,  75951,  89662,  90744,  90835,  91200,  92936,  94176, 100264,
        100369, 103072, 103464, 109985, 113394, 113596, 113869, 114317, 122767,
        128396, 128434, 128808, 128872, 128956, 132146, 134898, 137642, 137656,
        137673, 137712, 137859, 137970, 138204, 139181, 139219, 139504, 139527,
        139553, 139606, 142241, 142364, 142389, 142418, 142432, 142447, 142504,
        142542, 142617, 142632, 142674, 142712, 142771, 142835, 142839, 143793,
        144009, 144114, 149004, 150172, 150503, 152293, 152485, 152554, 152572,
        152599, 152684, 152712, 156069, 156091, 156166, 156284, 156328, 156329,
        156352, 156377, 156926, 157194, 158666, 158776, 158828, 163368, 166821,
        167405, 167534, 167745, 168972, 169063, 169614, 170957, 177710, 178084,
        179198, 179658, 179708, 182664, 184131, 186424, 187914, 194403, 194418,
        194447, 194464, 194615, 194777, 194920, 194980, 195051, 203661, 203669,
        204230, 206784, 207115, 212629, 221258, 241675, 244180, 244885, 245203,
        245419, 254220, 263014, 289957, 297673, 298130, 298157, 298189, 298327,
        298669, 309887, 310450, 314972, 314974, 314997, 315164, 316608, 316626,
        316732, 316767, 317111, 317376, 317541, 317828, 317931, 318949, 330765,
        333410, 334131, 334272, 334470, 334616, 339924, 340385, 340508, 340657,
        340674, 346002, 346056, 351023, 351096, 351672, 352632, 353138, 353406,
        353453, 353624, 353671, 354237, 356048, 357318, 357366, 358753, 358780,
        358788, 358899, 358902, 358934, 359065, 359236, 359328, 362634, 371408,
        372766, 372980, 375838, 376440, 377214, 382917, 382976, 383279, 383405,
        384060, 384097, 384198, 384260, 384423, 390983, 391032, 402480, 426151,
        426529, 433063, 436903, 445817, 445864, 460483, 460593, 460613, 467874,
        468338, 468690, 469228, 473954, 474485, 474605, 479330, 479797, 479875,
        479890, 479908, 479955, 482506, 482731, 482932, 482948, 483234, 483434,
        483508, 483677, 483808, 483819, 483893, 483904, 484026, 486329, 486336,
        486425, 486439, 486445, 486592, 486653, 486696, 486726, 486894, 492310,
        493012, 493191, 495016, 499397, 499624, 499928, 501717, 506308, 506330,
        506347, 506450, 506573, 506648, 514654, 520726, 525643, 526921, 527021,
        527272, 527342, 527375, 527394, 527451, 527456, 527457, 531019, 533004,
        533243, 533502, 547051, 548517, 548748, 565880, 567604, 567649, 567763,
        567850, 568005, 568248, 584453, 584502, 584736, 588684],
       device='cuda:0')}, 'encoder.layer_stack.1.slf_attn.w_vs': {'weight': tensor([     0,      6,      8,  ..., 589816, 589817, 589822], device='cuda:0')}, 'encoder.layer_stack.1.slf_attn.fc': {'weight': tensor([    13,     16,     17,  ..., 589799, 589811, 589817], device='cuda:0')}, 'encoder.layer_stack.1.pos_ffn.w_1': {'weight': tensor([      5,     682,     770,  ..., 2359238, 2359285, 2359289],
       device='cuda:0')}, 'encoder.layer_stack.1.pos_ffn.w_2': {'weight': tensor([     41,      47,      53,  ..., 2359200, 2359250, 2359292],
       device='cuda:0')}, 'preclassifier': {'weight': tensor([   770,    771,    783,  ..., 589800, 589813, 589819], device='cuda:0')}, 'slot_preclassifier': {'weight': tensor([     0,      4,     13,  ..., 589078, 589481, 589565], device='cuda:0')}, 'classifier.3': {'weight': tensor([    1,     2,     6,  ..., 16878, 16880, 16888], device='cuda:0')}, 'slot_classifier.3': {'weight': tensor([   15,    17,    40,  ..., 92897, 92906, 92925], device='cuda:0')}}
=> Using a preset learning rate schedule:
{0: 0.001, 5: 0.0001, 10: 1e-05}
