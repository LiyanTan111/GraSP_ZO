Namespace(batch_size=64, config='configs/ATIS/GraSP/SCD.yml', d_inner_hid=2048, d_k=64, d_model=768, d_v=64, dropout=0.1, embs_share_weight=False, epoch=100, label_smoothing=True, log=None, n_head=8, n_layers=6, n_warmup_steps=40000, no_cuda=False, ops_idx=None, precondition=0, proj_share_weight=True, quantized=1, save_mode='best', save_model=None, src_vocab_size=19213, tensorized=0, tgt_vocab_size=10839, uncompressed=0)
Transformer_sentence_concat_SLU(
  (encoder): Encoder(
    (src_word_emb): Embedding(800, 768)
    (position_enc): Embedding(512, 768)
    (token_type_emb): Embedding(2, 768)
    (dropout): Dropout(p=0.1, inplace=False)
    (layer_stack): ModuleList(
      (0): EncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=768, out_features=768, bias=True)
          (w_ks): Linear(in_features=768, out_features=768, bias=True)
          (w_vs): Linear(in_features=768, out_features=768, bias=True)
          (fc): Linear(in_features=768, out_features=768, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=3072, bias=True)
          (w_2): Linear(in_features=3072, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (relu): GELU(approximate='none')
        )
      )
      (1): EncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=768, out_features=768, bias=True)
          (w_ks): Linear(in_features=768, out_features=768, bias=True)
          (w_vs): Linear(in_features=768, out_features=768, bias=True)
          (fc): Linear(in_features=768, out_features=768, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=3072, bias=True)
          (w_2): Linear(in_features=3072, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (relu): GELU(approximate='none')
        )
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
  )
  (preclassifier): Linear(in_features=768, out_features=768, bias=True)
  (slot_preclassifier): Linear(in_features=768, out_features=768, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=768, out_features=768, bias=True)
    (1): Tanh()
    (2): Dropout(p=0.1, inplace=False)
    (3): Linear(in_features=768, out_features=22, bias=True)
  )
  (slot_classifier): Sequential(
    (0): Linear(in_features=768, out_features=768, bias=True)
    (1): GELU(approximate='none')
    (2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (3): Linear(in_features=768, out_features=121, bias=True)
  )
)
0
=> config.summary_dir:    ./runs/pruning/ATIS/attn/ZO_SCD_sign/GraSP_attn/scratch_95_grad0.5_lr0.001/20230328-001036/run_None/
=> config.checkpoint_dir: ./runs/pruning/ATIS/attn/ZO_SCD_sign/GraSP_attn/scratch_95_grad0.5_lr0.001/20230328-001036/run_None/
=> Applying weight initialization(kaiming).
Iteration of: 0/1
(1): Iterations 0/1.
(2): Iterations 0/1.
(2): Iterations 1/1.
** norm factor: tensor(0.2897, device='cuda:0')
** accept:  tensor(-1.3433e-05, device='cuda:0')
tensor(772264, device='cuda:0')
=> Using GraSP
{'Total': 16479119, 'Trainable': 16479119}
{'encoder.layer_stack.0.slf_attn.w_qs': {'weight': tensor([  1680,   1997,   7979,  ..., 589379, 589389, 589509], device='cuda:0')}, 'encoder.layer_stack.0.slf_attn.w_ks': {'weight': tensor([   794,   1072,   1161,  ..., 587505, 587673, 588475], device='cuda:0')}, 'encoder.layer_stack.0.slf_attn.w_vs': {'weight': tensor([     0,      4,      9,  ..., 589792, 589817, 589822], device='cuda:0')}, 'encoder.layer_stack.0.slf_attn.fc': {'weight': tensor([    71,    127,    132,  ..., 589738, 589742, 589744], device='cuda:0')}, 'encoder.layer_stack.0.pos_ffn.w_1': {'weight': tensor([     12,      50,     115,  ..., 2358864, 2359164, 2359272],
       device='cuda:0')}, 'encoder.layer_stack.0.pos_ffn.w_2': {'weight': tensor([     11,      46,     100,  ..., 2359162, 2359238, 2359246],
       device='cuda:0')}, 'encoder.layer_stack.1.slf_attn.w_qs': {'weight': tensor([  2225,   9267,   9285,   9333,   9387,   9464,   9529,   9577,   9675,
          9860,  10782,  10795,  10902,  10967,  13908,  13937,  13966,  14019,
         14025,  14185,  14189,  14214,  14238,  14244,  14249,  14333,  14335,
         14340,  14450,  14461,  14579,  14585,  21157,  22819,  25337,  26121,
         26126,  26163,  26229,  26252,  26366,  26421,  26478,  26665,  26738,
         26873,  32441,  32470,  32812,  35826,  36281,  36377,  36509,  36559,
         36594,  36632,  36770,  36785,  36826,  37669,  39776,  40121,  42283,
         42738,  42832,  45053,  45345,  45750,  45751,  45826,  69429,  95681,
         99353,  99385, 108855, 115638, 115836, 124321, 130007, 130046, 130139,
        130202, 130229, 130273, 130329, 130553, 135219, 137114, 137126, 137220,
        137465, 137917, 139581, 141344, 141541, 141606, 141652, 141659, 141725,
        141760, 141765, 141935, 141948, 146763, 149806, 150073, 150113, 150258,
        150499, 155246, 155390, 155457, 155736, 155841, 155955, 156119, 156402,
        156532, 156642, 159064, 159266, 159384, 159401, 161212, 175244, 175274,
        175425, 175526, 175677, 175706, 175777, 175793, 175883, 176014, 176027,
        176087, 176286, 176498, 176691, 189938, 194465, 194969, 195004, 204696,
        208489, 217529, 221197, 221305, 225198, 226113, 226200, 226500, 226814,
        227052, 245001, 245009, 245025, 245080, 245162, 245430, 245618, 258185,
        272009, 272012, 272498, 281501, 281724, 288043, 288339, 288407, 288498,
        288741, 294281, 294345, 294435, 294446, 294483, 294510, 294563, 294581,
        294601, 294615, 294633, 294635, 294700, 294713, 294721, 294723, 294850,
        294872, 294975, 300709, 307211, 307243, 307385, 307415, 307487, 307659,
        307930, 311214, 312606, 313149, 313428, 316246, 316354, 323766, 328492,
        334754, 335022, 335214, 335269, 336024, 336292, 347307, 351027, 353383,
        353986, 354222, 354357, 354400, 354486, 354585, 354604, 358632, 358890,
        359008, 359330, 360362, 360613, 360614, 360818, 360865, 365989, 366060,
        366242, 374009, 386741, 399768, 404921, 405101, 405150, 405309, 408138,
        410850, 421490, 421570, 421608, 422137, 422585, 423144, 427860, 428312,
        429081, 436023, 438173, 440234, 445582, 445750, 445878, 445932, 445977,
        454249, 456235, 459573, 459756, 462696, 462818, 462903, 465433, 466082,
        467260, 467570, 473496, 474264, 474448, 474545, 474562, 477726, 478269,
        481394, 483212, 483292, 483438, 483479, 483517, 483564, 483653, 483729,
        483778, 483797, 485472, 485497, 485698, 485742, 485796, 485885, 486004,
        489015, 491571, 492057, 492093, 492099, 492259, 500556, 506044, 507317,
        507319, 507459, 508578, 508836, 508853, 508930, 509235, 523182, 523256,
        523682, 530938, 530997, 531101, 531108, 531152, 534069, 534219, 535307,
        553868, 553962, 554166, 554199, 556605, 558387, 558850, 562189, 562310,
        562410, 562537, 562583, 562584, 562586, 562595, 562597, 562667, 562690,
        562937, 573012, 584758, 584806, 585104, 586873, 586922, 587103, 587211],
       device='cuda:0')}, 'encoder.layer_stack.1.slf_attn.w_ks': {'weight': tensor([  9300,   9566,   9608,   9825,  10098,  10283,  10312,  10340,  10501,
         10507,  10722,  13874,  14202,  14347,  14402,  14545,  17195,  17392,
         17596,  19980,  20041,  20267,  20347,  25867,  26044,  28430,  28440,
         28467,  28808,  28847,  28853,  28874,  28887,  28899,  28962,  28969,
         31243,  31671,  34644,  34761,  34864,  35021,  35617,  37072,  39348,
         39495,  39506,  39546,  39560,  39613,  39723,  42291,  42530,  42590,
         42786,  42787,  42819,  42868,  48316,  48683,  48719,  49122,  56525,
         67799,  68156,  75272,  75744,  75951,  89662,  91077, 100195, 100264,
        103072, 103464, 103642, 113475, 113869, 114317, 122767, 128434, 128779,
        128808, 128872, 128956, 132146, 134748, 134898, 137656, 137668, 137673,
        137712, 137970, 138204, 139181, 139278, 139363, 139418, 139473, 139504,
        139606, 142229, 142241, 142271, 142295, 142347, 142389, 142428, 142432,
        142504, 142542, 142564, 142603, 142617, 142632, 142771, 142839, 144195,
        149004, 150067, 150172, 150503, 152122, 152293, 152316, 152395, 152485,
        152599, 152684, 152712, 152715, 155990, 156156, 156185, 156284, 156329,
        156352, 156536, 156599, 157194, 158828, 158907, 167404, 167405, 168972,
        169036, 169063, 169391, 169614, 169921, 178084, 178326, 179198, 179670,
        179708, 180132, 183306, 183929, 186026, 186424, 187684, 187759, 187914,
        194338, 194403, 194447, 194454, 194464, 194615, 194920, 194980, 195051,
        200737, 203661, 204230, 207115, 207253, 212426, 212629, 221258, 229079,
        231999, 241337, 241675, 244180, 244885, 245419, 252095, 254220, 254785,
        262704, 263014, 263345, 264526, 282728, 282765, 283215, 283232, 289989,
        295116, 295122, 295588, 297289, 297490, 298130, 298189, 298472, 298669,
        301836, 310450, 314892, 314974, 314997, 315021, 315058, 315105, 315156,
        315164, 315640, 316626, 316732, 317196, 317376, 317541, 317828, 317931,
        318949, 322876, 323646, 330852, 334131, 334272, 334373, 334470, 334616,
        340508, 340657, 346056, 350576, 351023, 353138, 353406, 353624, 357366,
        358780, 358788, 358934, 359152, 359154, 359155, 359236, 359317, 359328,
        359400, 360648, 362771, 375838, 376162, 376440, 381971, 382976, 383279,
        384423, 384721, 390983, 391032, 402480, 407803, 426151, 426513, 433063,
        436903, 444675, 445817, 445864, 445875, 446520, 446947, 458780, 460035,
        460242, 460246, 460483, 460525, 460593, 467874, 468001, 468201, 468305,
        468338, 468438, 468451, 468462, 468690, 469228, 473954, 474145, 474485,
        474605, 479330, 479391, 479532, 479557, 479604, 479619, 479657, 479797,
        479857, 479908, 482177, 482593, 483234, 483349, 483434, 483677, 483819,
        483893, 483904, 483986, 483999, 484026, 484121, 484142, 484227, 486336,
        486421, 486425, 486439, 486445, 486592, 486696, 486726, 486894, 490610,
        492562, 493012, 495016, 496511, 499217, 499383, 499397, 499624, 499783,
        499865, 499928, 501717, 502112, 503394, 506308, 506327, 506330, 506347,
        506450, 506573, 506648, 514654, 520905, 525643, 526932, 527021, 527026,
        527079, 527115, 527272, 527342, 527375, 527394, 527431, 527451, 527456,
        527457, 529215, 529332, 531019, 533004, 533076, 533243, 533502, 533645,
        547051, 548599, 548748, 555511, 557035, 560268, 560295, 563408, 565112,
        565880, 567604, 567605, 567649, 567763, 567908, 567935, 568005, 568086,
        568196, 568608, 571008, 571268, 583335, 584453, 584502, 584736, 585087,
        585139, 585750, 588684, 589700], device='cuda:0')}, 'encoder.layer_stack.1.slf_attn.w_vs': {'weight': tensor([     0,      6,      8,  ..., 589816, 589817, 589822], device='cuda:0')}, 'encoder.layer_stack.1.slf_attn.fc': {'weight': tensor([    13,     16,     17,  ..., 589799, 589811, 589817], device='cuda:0')}, 'encoder.layer_stack.1.pos_ffn.w_1': {'weight': tensor([      5,     770,     774,  ..., 2359238, 2359285, 2359289],
       device='cuda:0')}, 'encoder.layer_stack.1.pos_ffn.w_2': {'weight': tensor([     26,      41,      47,  ..., 2359247, 2359250, 2359292],
       device='cuda:0')}, 'preclassifier': {'weight': tensor([   770,    771,    783,  ..., 589800, 589813, 589819], device='cuda:0')}, 'slot_preclassifier': {'weight': tensor([     0,      4,     13,  ..., 588993, 589035, 589038], device='cuda:0')}, 'classifier.3': {'weight': tensor([    1,     2,     5,  ..., 16880, 16888, 16891], device='cuda:0')}, 'slot_classifier.3': {'weight': tensor([   15,    17,    40,  ..., 92897, 92906, 92925], device='cuda:0')}}
=> Using a preset learning rate schedule:
{0: 0.001, 10: 0.0001, 20: 1e-05}
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            loss =  tensor(0.1174, device='cuda:0')
acc =  tensor(0.7406, device='cuda:0')
torch.cuda.memory_allocated: 1.209158GB
loss =  tensor(0.1164, device='cuda:0')
acc =  tensor(0.7396, device='cuda:0')
torch.cuda.memory_allocated: 1.209158GB
loss =  tensor(0.1155, device='cuda:0')
acc =  tensor(0.7321, device='cuda:0')
torch.cuda.memory_allocated: 1.209158GB
loss =  tensor(0.1144, device='cuda:0')
acc =  tensor(0.7383, device='cuda:0')
torch.cuda.memory_allocated: 1.209158GB
loss =  tensor(0.1134, device='cuda:0')
acc =  tensor(0.7378, device='cuda:0')
torch.cuda.memory_allocated: 1.209158GB
loss =  tensor(0.1123, device='cuda:0')
acc =  tensor(0.7453, device='cuda:0')
torch.cuda.memory_allocated: 1.209158GB
loss =  tensor(0.1114, device='cuda:0')
acc =  tensor(0.7443, device='cuda:0')
torch.cuda.memory_allocated: 1.209158GB
loss =  tensor(0.1105, device='cuda:0')
acc =  tensor(0.7409, device='cuda:0')
torch.cuda.memory_allocated: 1.209158GB
loss =  tensor(0.1095, device='cuda:0')
acc =  tensor(0.7380, device='cuda:0')
torch.cuda.memory_allocated: 1.209158GB
