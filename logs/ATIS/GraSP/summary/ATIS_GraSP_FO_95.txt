Namespace(batch_size=64, config='configs/ATIS/GraSP/FO.yml', d_inner_hid=2048, d_k=64, d_model=768, d_v=64, dropout=0.1, embs_share_weight=False, epoch=100, label_smoothing=True, log=None, n_head=8, n_layers=6, n_warmup_steps=40000, no_cuda=False, ops_idx=None, precondition=0, proj_share_weight=True, quantized=1, save_mode='best', save_model=None, src_vocab_size=19213, tensorized=0, tgt_vocab_size=10839, uncompressed=0)
Transformer_sentence_concat_SLU(
  (encoder): Encoder(
    (src_word_emb): Embedding(800, 768)
    (position_enc): Embedding(512, 768)
    (token_type_emb): Embedding(2, 768)
    (dropout): Dropout(p=0.1, inplace=False)
    (layer_stack): ModuleList(
      (0): EncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=768, out_features=768, bias=True)
          (w_ks): Linear(in_features=768, out_features=768, bias=True)
          (w_vs): Linear(in_features=768, out_features=768, bias=True)
          (fc): Linear(in_features=768, out_features=768, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=3072, bias=True)
          (w_2): Linear(in_features=3072, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (relu): GELU(approximate='none')
        )
      )
      (1): EncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=768, out_features=768, bias=True)
          (w_ks): Linear(in_features=768, out_features=768, bias=True)
          (w_vs): Linear(in_features=768, out_features=768, bias=True)
          (fc): Linear(in_features=768, out_features=768, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=3072, bias=True)
          (w_2): Linear(in_features=3072, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (relu): GELU(approximate='none')
        )
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
  )
  (preclassifier): Linear(in_features=768, out_features=768, bias=True)
  (slot_preclassifier): Linear(in_features=768, out_features=768, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=768, out_features=768, bias=True)
    (1): Tanh()
    (2): Dropout(p=0.1, inplace=False)
    (3): Linear(in_features=768, out_features=22, bias=True)
  )
  (slot_classifier): Sequential(
    (0): Linear(in_features=768, out_features=768, bias=True)
    (1): GELU(approximate='none')
    (2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (3): Linear(in_features=768, out_features=121, bias=True)
  )
)
0
=> config.summary_dir:    ./runs/pruning/ATIS/attn/ADAM/ATIS_GraSP_FO_95/run_None/summary/
=> config.checkpoint_dir: ./runs/pruning/ATIS/attn/ADAM/ATIS_GraSP_FO_95/run_None/checkpoint/
{'Total': 16479119, 'Trainable': 16479119}
=> Applying weight initialization(kaiming).
Iteration of: 0/1
(1): Iterations 0/1.
(2): Iterations 0/1.
(2): Iterations 1/1.
** norm factor: tensor(0.2894, device='cuda:0')
** accept:  tensor(-1.3451e-05, device='cuda:0')
tensor(772264, device='cuda:0')
=> Using GraSP
=> Using a preset learning rate schedule:
{0: 0.001, 20: 0.0001, 50: 1e-05}

epoch =  0
  - (Training)   loss:  0.05472, loss_hard:  0.00000, accuracy: 73.018 %, slot accuracy: 72.315,elapse: 0.053 min
  - (Validation) loss:  0.04844, accuracy: 71.400 %, slot accuracy: 82.027,elapse: 0.007 min
  - (Test) loss:  0.04995, accuracy: 70.773 %, slot accuracy: 80.947,elapse: 0.007 min

epoch =  1
  - (Training)   loss:  0.04196, loss_hard:  0.00000, accuracy: 74.626 %, slot accuracy: 86.056,elapse: 0.054 min
  - (Validation) loss:  0.04223, accuracy: 73.600 %, slot accuracy: 86.744,elapse: 0.007 min
  - (Test) loss:  0.04284, accuracy: 71.445 %, slot accuracy: 86.600,elapse: 0.007 min

epoch =  2
  - (Training)   loss:  0.03595, loss_hard:  0.00000, accuracy: 79.495 %, slot accuracy: 89.520,elapse: 0.056 min
  - (Validation) loss:  0.03729, accuracy: 80.000 %, slot accuracy: 89.286,elapse: 0.008 min
  - (Test) loss:  0.03766, accuracy: 78.275 %, slot accuracy: 88.488,elapse: 0.008 min

epoch =  3
  - (Training)   loss:  0.03319, loss_hard:  0.00000, accuracy: 85.035 %, slot accuracy: 90.863,elapse: 0.056 min
  - (Validation) loss:  0.03548, accuracy: 82.000 %, slot accuracy: 89.356,elapse: 0.008 min
  - (Test) loss:  0.03596, accuracy: 82.307 %, slot accuracy: 89.382,elapse: 0.008 min

epoch =  4
  - (Training)   loss:  0.03141, loss_hard:  0.00000, accuracy: 87.871 %, slot accuracy: 91.338,elapse: 0.058 min
  - (Validation) loss:  0.03413, accuracy: 83.400 %, slot accuracy: 90.058,elapse: 0.008 min
  - (Test) loss:  0.03455, accuracy: 82.531 %, slot accuracy: 89.666,elapse: 0.008 min

epoch =  5
  - (Training)   loss:  0.03024, loss_hard:  0.00000, accuracy: 89.390 %, slot accuracy: 92.195,elapse: 0.059 min
  - (Validation) loss:  0.03325, accuracy: 86.400 %, slot accuracy: 90.672,elapse: 0.008 min
  - (Test) loss:  0.03406, accuracy: 84.658 %, slot accuracy: 90.201,elapse: 0.008 min
