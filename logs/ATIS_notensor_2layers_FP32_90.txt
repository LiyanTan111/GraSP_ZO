Namespace(batch_size=32, config='configs/ATIS/GraSP_90.yml', d_inner_hid=2048, d_k=64, d_model=768, d_v=64, dropout=0.1, embs_share_weight=False, epoch=500, label_smoothing=True, log=None, n_head=8, n_layers=6, n_warmup_steps=40000, no_cuda=False, ops_idx=None, precondition=0, proj_share_weight=True, quantized=1, save_mode='best', save_model='model/ATIS_notensor_2layers_FP32_90', src_vocab_size=19213, tensorized=0, tgt_vocab_size=10839, uncompressed=0)
Transformer_sentence_concat_SLU(
  (encoder): Encoder(
    (src_word_emb): Embedding(800, 768)
    (position_enc): Embedding(512, 768)
    (token_type_emb): Embedding(2, 768)
    (dropout): Dropout(p=0.1, inplace=False)
    (layer_stack): ModuleList(
      (0): EncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=768, out_features=768, bias=True)
          (w_ks): Linear(in_features=768, out_features=768, bias=True)
          (w_vs): Linear(in_features=768, out_features=768, bias=True)
          (fc): Linear(in_features=768, out_features=768, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=3072, bias=True)
          (w_2): Linear(in_features=3072, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (relu): GELU(approximate='none')
        )
      )
      (1): EncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=768, out_features=768, bias=True)
          (w_ks): Linear(in_features=768, out_features=768, bias=True)
          (w_vs): Linear(in_features=768, out_features=768, bias=True)
          (fc): Linear(in_features=768, out_features=768, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=3072, bias=True)
          (w_2): Linear(in_features=3072, out_features=768, bias=True)
          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (relu): GELU(approximate='none')
        )
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
  )
  (preclassifier): Linear(in_features=768, out_features=768, bias=True)
  (slot_preclassifier): Linear(in_features=768, out_features=768, bias=True)
  (classifier): Sequential(
    (0): Linear(in_features=768, out_features=768, bias=True)
    (1): Tanh()
    (2): Dropout(p=0.1, inplace=False)
    (3): Linear(in_features=768, out_features=22, bias=True)
  )
  (slot_classifier): Sequential(
    (0): Linear(in_features=768, out_features=768, bias=True)
    (1): GELU(approximate='none')
    (2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (3): Linear(in_features=768, out_features=121, bias=True)
  )
)
0
=> config.summary_dir:    ./runs/pruning/ATIS/ATIS_GraSP_prune_ratio_90/run_None/summary/
=> config.checkpoint_dir: ./runs/pruning/ATIS/ATIS_GraSP_prune_ratio_90/run_None/checkpoint/
encoder.src_word_emb.weight : torch.Size([800, 768])
encoder.position_enc.weight : torch.Size([512, 768])
encoder.token_type_emb.weight : torch.Size([2, 768])
encoder.layer_stack.0.slf_attn.w_qs.weight : torch.Size([768, 768])
encoder.layer_stack.0.slf_attn.w_qs.bias : torch.Size([768])
encoder.layer_stack.0.slf_attn.w_ks.weight : torch.Size([768, 768])
encoder.layer_stack.0.slf_attn.w_ks.bias : torch.Size([768])
encoder.layer_stack.0.slf_attn.w_vs.weight : torch.Size([768, 768])
encoder.layer_stack.0.slf_attn.w_vs.bias : torch.Size([768])
encoder.layer_stack.0.slf_attn.fc.weight : torch.Size([768, 768])
encoder.layer_stack.0.slf_attn.fc.bias : torch.Size([768])
encoder.layer_stack.0.slf_attn.layer_norm.weight : torch.Size([768])
encoder.layer_stack.0.slf_attn.layer_norm.bias : torch.Size([768])
encoder.layer_stack.0.pos_ffn.w_1.weight : torch.Size([3072, 768])
encoder.layer_stack.0.pos_ffn.w_1.bias : torch.Size([3072])
encoder.layer_stack.0.pos_ffn.w_2.weight : torch.Size([768, 3072])
encoder.layer_stack.0.pos_ffn.w_2.bias : torch.Size([768])
encoder.layer_stack.0.pos_ffn.layer_norm.weight : torch.Size([768])
encoder.layer_stack.0.pos_ffn.layer_norm.bias : torch.Size([768])
encoder.layer_stack.1.slf_attn.w_qs.weight : torch.Size([768, 768])
encoder.layer_stack.1.slf_attn.w_qs.bias : torch.Size([768])
encoder.layer_stack.1.slf_attn.w_ks.weight : torch.Size([768, 768])
encoder.layer_stack.1.slf_attn.w_ks.bias : torch.Size([768])
encoder.layer_stack.1.slf_attn.w_vs.weight : torch.Size([768, 768])
encoder.layer_stack.1.slf_attn.w_vs.bias : torch.Size([768])
encoder.layer_stack.1.slf_attn.fc.weight : torch.Size([768, 768])
encoder.layer_stack.1.slf_attn.fc.bias : torch.Size([768])
encoder.layer_stack.1.slf_attn.layer_norm.weight : torch.Size([768])
encoder.layer_stack.1.slf_attn.layer_norm.bias : torch.Size([768])
encoder.layer_stack.1.pos_ffn.w_1.weight : torch.Size([3072, 768])
encoder.layer_stack.1.pos_ffn.w_1.bias : torch.Size([3072])
encoder.layer_stack.1.pos_ffn.w_2.weight : torch.Size([768, 3072])
encoder.layer_stack.1.pos_ffn.w_2.bias : torch.Size([768])
encoder.layer_stack.1.pos_ffn.layer_norm.weight : torch.Size([768])
encoder.layer_stack.1.pos_ffn.layer_norm.bias : torch.Size([768])
encoder.layer_norm.weight : torch.Size([768])
encoder.layer_norm.bias : torch.Size([768])
preclassifier.weight : torch.Size([768, 768])
preclassifier.bias : torch.Size([768])
slot_preclassifier.weight : torch.Size([768, 768])
slot_preclassifier.bias : torch.Size([768])
classifier.3.weight : torch.Size([22, 768])
classifier.3.bias : torch.Size([22])
slot_classifier.2.weight : torch.Size([768])
slot_classifier.2.bias : torch.Size([768])
slot_classifier.3.weight : torch.Size([121, 768])
slot_classifier.3.bias : torch.Size([121])
{'Total': 16479119, 'Trainable': 16479119}
=> Applying weight initialization(kaiming).
Iteration of: 0/1
(1): Iterations 0/1.
(2): Iterations 0/1.
(2): Iterations 1/1.
** norm factor: tensor(0.2478, device='cuda:0')
** accept:  tensor(-7.1131e-06, device='cuda:0')
tensor(1544526, device='cuda:0')
=> Using GraSP
=> Using a preset learning rate schedule:
{0: 0.001, 100: 0.0001, 150: 1e-05}

epoch =  0
  - (Training)   loss:  0.08796, loss_hard:  0.00000, accuracy: 75.631 %, slot accuracy: 82.185,elapse: 0.071 min
  - (Validation) loss:  0.07711, accuracy: 77.200 %, slot accuracy: 87.270,elapse: 0.008 min
  - (Test) loss:  0.07711, accuracy: 76.932 %, slot accuracy: 86.851,elapse: 0.008 min

epoch =  1
  - (Training)   loss:  0.06542, loss_hard:  0.00000, accuracy: 86.710 %, slot accuracy: 89.932,elapse: 0.071 min
  - (Validation) loss:  0.06905, accuracy: 84.400 %, slot accuracy: 88.129,elapse: 0.008 min
  - (Test) loss:  0.07090, accuracy: 82.979 %, slot accuracy: 88.608,elapse: 0.008 min

epoch =  2
  - (Training)   loss:  0.06027, loss_hard:  0.00000, accuracy: 90.060 %, slot accuracy: 91.150,elapse: 0.059 min
  - (Validation) loss:  0.06384, accuracy: 89.600 %, slot accuracy: 90.882,elapse: 0.007 min
  - (Test) loss:  0.06419, accuracy: 87.010 %, slot accuracy: 90.561,elapse: 0.007 min

epoch =  3
  - (Training)   loss:  0.05633, loss_hard:  0.00000, accuracy: 92.942 %, slot accuracy: 92.536,elapse: 0.060 min
  - (Validation) loss:  0.05967, accuracy: 91.400 %, slot accuracy: 91.846,elapse: 0.007 min
  - (Test) loss:  0.06307, accuracy: 88.130 %, slot accuracy: 91.543,elapse: 0.007 min

epoch =  4
  - (Training)   loss:  0.05344, loss_hard:  0.00000, accuracy: 95.354 %, slot accuracy: 94.265,elapse: 0.061 min
  - (Validation) loss:  0.05684, accuracy: 93.800 %, slot accuracy: 93.460,elapse: 0.007 min
  - (Test) loss:  0.06022, accuracy: 91.265 %, slot accuracy: 92.973,elapse: 0.007 min

epoch =  5
  - (Training)   loss:  0.05117, loss_hard:  0.00000, accuracy: 96.471 %, slot accuracy: 95.592,elapse: 0.066 min
  - (Validation) loss:  0.05375, accuracy: 95.800 %, slot accuracy: 94.687,elapse: 0.007 min
  - (Test) loss:  0.05769, accuracy: 90.817 %, slot accuracy: 94.413,elapse: 0.007 min

epoch =  6
  - (Training)   loss:  0.04957, loss_hard:  0.00000, accuracy: 97.521 %, slot accuracy: 96.521,elapse: 0.064 min
  - (Validation) loss:  0.05372, accuracy: 96.000 %, slot accuracy: 94.494,elapse: 0.007 min
  - (Test) loss:  0.05729, accuracy: 92.497 %, slot accuracy: 94.053,elapse: 0.007 min
